---
title: "Building the Perfect Soccer Player"
author: "team-devils"
date: "Dec 14, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load_packages, include = FALSE}
library(tidyverse)
library(broom)
library(rvest)
library(infer)
library(modelr)
```

## Introduction






## Data Analysis

```{r load-data, include = FALSE}
players1 <- read_csv("../data/players1.csv")
```

To get a general sense of the market values of all players in the 2018-2019 season, let's first create a histogram to visualize their distribution. 

```{r histogram, fig.width = 10}
players1 %>%
  ggplot(mapping = aes(market_value, fill = ..count..)) +
  geom_histogram(binwidth = 5) +
  labs(
    title = "Distribution of Player Market Values in 2018-2019 Season",
    x = "Player Market Values in 2018-2019 season", 
    y = "Count"
  ) +
  theme_minimal() +
  scale_fill_gradient(low="blue", high="red")
```

In the histogram above, we can see that the distribution of the players' market values in the 2018-2019 season is right skewed, and that the most commonly occuring market values are slightly less than $25 million. In the summary statistics below, we can see that the mean of the market values is higher than the most commonly occuring market values due to the right skewedness of the data. 

```{r summary-stats}
players1 %>%
  summarise(mean = mean(market_value), median = median(market_value), sd = sd(market_value), min = min(market_value), max = max(market_value))

```

The median, however, seems to be affected to a lesser extent by the high valued outliers, and we can see this in the boxplot below. 

```{r boxplot, fig.height = 5, fig.width = 10}
players1 %>%
  ggplot(mapping = aes(y = market_value)) +
  geom_boxplot() +
  coord_flip() +
  labs(
    title = "Boxplot of Player Market Values in 2018-2019 Season",
    y = "Player Market Values in 2018-2019 season"
  ) +
  theme_minimal() 
  

players1 %>%
  select(name, market_value) %>%
  filter(market_value > 80)

```


```{r select-data}
players1 %>%
  group_by(position_new) %>%
  summarise(goals_avg = mean(goals))
players1 %>%
  group_by(position_new) %>%
  summarise(assists_avg = mean(assists))
```


```{r predict}
linear_prediction  <- lm(market_value ~ position_new + age + matches + goals + own_goals +
                  assists + yellow_cards + red_cards + substituted_on +
                  substituted_off + age_range + position_new * goals + position_new * assists, data =   players1)
tidy(linear_prediction)
```

To begin with, we try to make a multiple linear regression based on all the statistical variables we have in the players dataset. In soccer, player's position is highly correlated with his performances, especially goals and assists (e.g. Centre-Forwards get most goals and Midfielders make most assists in general while Goalkeepers can seldom score a goal or make an assist). Therefore, we managed to introduce two interactions between position/goals and position/assists into our multiple linear model.

```{r AIC-selection, include = FALSE}
selected_model <- step(linear_prediction, direction = "backward")
```

```{r glance-AIC}
tidy(selected_model)


glance(linear_prediction)$AIC
glance(selected_model)$AIC
```

Through the model selection based on AIC, we can see that the variables "age", "own_goals", "yellow_cards", "red_cards", "substituted_on", substituted_off" are eliminated. The AIC is reduced compared to the previous dataset (4513.152 to 4504.674), which can be interpreted as the increased likelihood of the model.

```{r r-squared}
glance(selected_model)$r.squared
```

```{r cross_validation}
test_cv <- crossv_kfold(players1, 10)
models <- map(test_cv$train, ~ selected_model)
rmses <- map2_dbl(models, test_cv$test, rmse)
rmses
```




## Conclusion

Your project goes here! Before you submit, make sure your chunks are turned off with `echo = FALSE`. 

You can add sections as you see fit. Make sure you have a section called Introduction at the beginning and a section called Conclusion at the end. The rest is up to you!
